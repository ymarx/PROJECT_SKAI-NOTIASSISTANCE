# Model configuration for SKAI-NotiAssistance

# LLM Configuration
llm:
  # 사용할 언어 모델 제공자(provider)를 선택합니다.
  # 옵션:
  # - 'local': 로컬 컴퓨터에 저장된 모델 사용 (Transformers 라이브러리 필요)
  # - 'ollama': 로컬에서 실행 중인 Ollama API 서버 사용 (Ollama 설치 필요)
  # - 'openai': OpenAI API 사용 (API 키 필요)
  # - 'huggingface': Hugging Face 모델 (로컬 또는 Inference API, 추가 설정 필요 가능)
  # - 'anthropic': Anthropic API 사용 (API 키 및 추가 설정 필요 가능)
  provider: "local"  # <--- 사용할 Provider 선택 (예: local, ollama, openai)

  # 선택한 provider에 따라 해석되는 모델 이름 또는 식별자입니다.
  # - provider: 'local' -> 아래 'model_path'에 지정된 모델을 로드하므로 여기는 참조용 이름 또는 경로 일부.
  # - provider: 'ollama' -> Ollama에 설치된 모델 태그 (예: "llama3:8b", "mistral:latest")
  # - provider: 'openai' -> OpenAI 모델 이름 (예: "gpt-4", "gpt-3.5-turbo")
  # - provider: 'huggingface' -> Hugging Face Hub 모델 이름 (예: "google/flan-t5-large")
  model_name: "KoAlpaca-Polyglot-5.8B" # <--- 사용할 모델 이름/태그 지정

  # --- Provider별 추가 설정 ---
  # provider: 'local' 일 때 필수 설정
  model_path: "path/to/your/local/llm/KoAlpaca-Polyglot-5.8B" # <--- 로컬 모델 파일이 있는 실제 경로 지정!

  # provider: 'ollama' 일 때 필수 설정
  api_url: "http://localhost:11434/api/generate" # 로컬 Ollama 서버 주소 (기본값)

  # provider: 'openai' 또는 'anthropic' 등 API 사용 시 필요할 수 있음
  # api_key: "YOUR_API_KEY" # 직접 지정하거나 환경 변수(OPENAI_API_KEY 등) 사용 권장

  # --- 일반 LLM 생성 파라미터 ---
  temperature: 0.1    # 생성 텍스트의 창의성 조절 (0.0 ~ 1.0). 낮을수록 결정적, 높을수록 다양함. (기본값: 0.1 권장)
  max_tokens: 2048    # 한 번의 요청으로 생성할 최대 토큰 수. 모델과 작업에 따라 조절 필요.
  top_p: 1.0          # 상위 확률 샘플링 파라미터 (0.0 ~ 1.0). 보통 1.0 사용.
  frequency_penalty: 0 # 같은 단어 반복 패널티 (0 ~ 2).
  presence_penalty: 0  # 새로운 주제 등장 장려 패널티 (0 ~ 2).
  timeout: 120        # API 요청 시 최대 대기 시간 (초). 로컬 모델은 해당 없음.

# Embedding Configuration
embedding:
  # 사용할 임베딩 모델의 이름 또는 식별자입니다.
  # ModelManager가 이 이름을 사용하여 적절한 임베딩 생성 방식을 결정합니다.
  # 예시:
  # - 로컬 Sentence Transformer 모델: "jhgan/ko-sroberta-multitask", "paraphrase-multilingual-mpnet-base-v2"
  #   (ModelManager 내 'huggingface' provider 로직에서 처리, SentenceTransformer 라이브러리 필요)
  # - OpenAI 임베딩 모델: "text-embedding-ada-002", "text-embedding-3-small"
  #   (ModelManager 내 'openai' provider 로직에서 처리, OpenAI API 키 필요)
  model_name: "jhgan/ko-sroberta-multitask" # <--- 사용할 임베딩 모델 지정

  # 임베딩 모델의 출력 벡터 차원 수입니다. **매우 중요!**
  # 사용하는 `embedding.model_name` 에 따라 정확한 값을 지정해야 합니다.
  # VectorStore(특히 FAISS, Pinecone) 설정 시 이 값이 사용됩니다.
  # 예시:
  # - "jhgan/ko-sroberta-multitask": 768
  # - "paraphrase-multilingual-mpnet-base-v2": 768
  # - "text-embedding-ada-002": 1536
  # - "text-embedding-3-small": 1536
  dimensions: 768 # <--- 사용하는 임베딩 모델의 실제 차원 수 지정!

  # OpenAI 등 API 기반 임베딩 사용 시, 한 번에 처리할 텍스트 배치 크기.
  # 로컬 모델 사용 시에는 덜 중요할 수 있습니다.
  batch_size: 64

# Vector Store Configuration
vector_store:
  # 사용할 벡터 저장소 종류를 선택합니다.
  # 옵션:
  # - 'chroma': 로컬 파일 기반 벡터 DB (설치 및 사용 간편) - 추천 시작점
  # - 'faiss': 로컬 파일 기반 고성능 벡터 검색 라이브러리 (설치가 다소 복잡할 수 있음)
  # - 'pinecone': 클라우드 기반 벡터 DB 서비스 (API 키, 환경 설정 필요)
  provider: "chroma"  # <--- 사용할 VectorStore 선택 (예: chroma, faiss)

  # 데이터를 저장/관리할 컬렉션(Chroma, Pinecone) 또는 인덱스(FAISS)의 이름입니다.
  collection_name: "skai_equipment_knowledge"

  # provider가 'chroma' 또는 'faiss' 인 경우, 데이터를 저장할 로컬 디렉토리 경로입니다.
  # 이 디렉토리가 없으면 자동으로 생성됩니다.
  persist_directory: "./data/vector_store" # <--- 데이터 저장 위치 지정

  # 벡터 간 유사도/거리 계산 방식을 지정합니다.
  # VectorStore provider 마다 지원하는 방식이 다를 수 있습니다.
  # - 'cosine': 코사인 유사도 (Chroma, Pinecone 등에서 주로 사용)
  # - 'l2': 유클리드 거리 (FAISS 기본값)
  # - 'ip': 내적 (FAISS 등에서 지원)
  # 선택한 provider가 지원하는 메트릭인지 확인 필요.
  distance_metric: "cosine" # <--- 사용할 거리/유사도 메트릭 지정

  # VectorStore 생성/로드 시 필요한 벡터 차원 수입니다.
  # 위의 `embedding.dimensions` 값과 **반드시 동일해야 합니다.**
  # VectorStore 초기화 시 이 값을 명시적으로 사용하여 오류를 방지합니다.
  dimensions: 768 # <--- embedding.dimensions와 동일한 값 지정!

# Inference Settings
inference:
  # LLM 응답을 스트리밍 방식으로 받을지 여부 (True/False).
  # ModelManager 및 사용하는 provider가 스트리밍을 지원해야 합니다.
  stream: False

  # LLM 호출 결과 등을 캐싱할지 여부 (True/False).
  # 캐싱 구현이 별도로 필요합니다 (예: utils/cache.py).
  cache_results: True

  # 캐시 유효 기간 (초). cache_results가 True일 때 사용됩니다.
  cache_ttl: 3600  # 1시간 (초 단위) 